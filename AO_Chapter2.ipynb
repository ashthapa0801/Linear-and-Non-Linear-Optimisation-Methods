{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "blvwmsIlpR9t"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D2H6Vmq8ScUD"
      },
      "source": [
        "# CHAPTER 2 - Statistical Estimation Problems"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "---\n",
        "\n",
        "**Author:** Dr Giordano Scarciotti (g.scarciotti@imperial.ac.uk) - Imperial College London \n",
        "\n",
        "**Module:** ELEC70066 - Advanced Optimisation\n",
        "\n",
        "**Version:** 1.2.0 - 04/01/2023\n",
        "\n",
        "---\n",
        "---"
      ],
      "metadata": {
        "id": "0MR1O6ZA7qKT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The material of this chapter is adapted from $[1]$."
      ],
      "metadata": {
        "id": "9XjvOu0-7spi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This chapter covers the class of applied convex optimisation problems in the fields of *statistical approximation*. The chapter consists of two sections: \n",
        "*  In Section 2.1 we formulate the maximum likelihood estimation problem, we link this problem to the approximation problem seen in Chapter 1 and then we see two examples: counting events and assessing the probability of getting a disease. \n",
        "*  In Section 2.2 we formulate the optimal detector problem to test hypotheses. We work out an example which shows how scalarisation by weighted sum does not necessarily provide all the optimal Pareto points."
      ],
      "metadata": {
        "id": "SCP9SKZI8BpY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Nota bene:** *The purpose of this chapter is to expose you immediately to important classes of applied optimisation problems to motivate the course. You should not expect to be able to grasp the mathematical details at this stage. Later in the course you will learn all the required knowledge to fully understand the theory of this chapter.*"
      ],
      "metadata": {
        "id": "l6-C5KVcMZjQ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p9JQK-JBSccX"
      },
      "source": [
        "# 2.1 Parametric Distribution Estimation"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import HTML\n",
        "HTML('<iframe width=\"850\" height=\"480\" src=\"https://www.youtube.com/embed/F0pKFI23VuA\"></iframe>')"
      ],
      "metadata": {
        "id": "nXEVBtkcRWEv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 504
        },
        "outputId": "d774fada-5ddc-4a61-c0f9-bbde77fcc71f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<iframe width=\"850\" height=\"480\" src=\"https://www.youtube.com/embed/F0pKFI23VuA\"></iframe>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Errata:** At 18:56 the video says \"optimal value\". It should be \"optimal point\" (i.e. solution).  "
      ],
      "metadata": {
        "id": "tjso8abPPxrv"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0x-D2zeAimTX"
      },
      "source": [
        "## 2.1.1 Maximum likelihood estimation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N_5y4s6USce_"
      },
      "source": [
        "Let $y$ be the number of traffic accidents in some region over some period, and $u_1$ be the total traffic flow through the region during the period, $u_2$ the rainfall in the region during the period, and so on. The counting of traffic accidents can be modelled by a [probability distribution](https://colab.research.google.com/drive/1WwNsPbW7-2PyVM_fXsOCUX7O7q8lQM3W#scrollTo=ZpyjDG4HTVEl&line=19&uniqifier=1), e.g. a [Poisson distribution](https://colab.research.google.com/drive/1WwNsPbW7-2PyVM_fXsOCUX7O7q8lQM3W#scrollTo=F3PtR1ZsU-RT), that relates the variables $y$ to the variables $u$. What we would like to estimate here are the parameters of the distribution so that given a new vector $u$ for day $j$ we can estimate the probability of having a certain number of accidents $y$ on that day. This is an example of **parametric distribution estimation**.\n",
        "\n",
        "Consider a family of probability distributions on $\\mathbb{R}^m$, indexed by a vector $x \\in \\mathbb{R}^n$, with [densities](https://colab.research.google.com/drive/1WwNsPbW7-2PyVM_fXsOCUX7O7q8lQM3W#scrollTo=ZpyjDG4HTVEl) $p_x(\\cdot)$. $x$ here represents the parameters of the distribution (e.g. mean, variance, or more complicated statistical quantities). When considered as a function of $x$, for a fixed observed data $y\\in\\mathbb{R}^m$, the function $p_x(y)$ is called the **likelihood function**. As we will see, it is more convenient to work with its logarithm, so we define the **log-likelihood function** \n",
        "\n",
        "$$\n",
        "l(x) = \\log p_x(y).\n",
        "$$\n",
        "\n",
        "Constraints that represent prior knowledge can either be considered explicitly (i.e. $x\\in C$) or incorporated in the likelihood function by assigning $p_x(y)=0$ whenever $x$ violate the prior information (thus the log-likelihood is $-\\infty$). The optimisation problem that we want to solve is the **maximum likelihood (ML) estimation problem**, which consists of solving\n",
        "\n",
        "$$\n",
        "\\begin{array}{ll}\n",
        "\\displaystyle\\max_x &\\log p_x(y)\\\\\n",
        "\\text{s.t.} &x\\in C.\n",
        "\\end{array}\\tag{1}\n",
        "$$\n",
        "\n",
        "In this problem $y$ is the observed data and $x$ is the variable (parameters of the distribution). Note that the maximiser of $\\log p_x(y)$ is the same as the maximiser of $p_x(y)$, which is why we can consider the $\\log$ without loss of generality. Problem $(1)$ is a convex optimisation problem if the log-likelihood function is concave for each $y$ and the $C$ is described by a set of linear equality and convex inequality constraints. This fact will be clear later in the course.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zii2NfROi2sP"
      },
      "source": [
        "## 2.1.2 Relation between ML and penalty function approximation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N10khLesixoP"
      },
      "source": [
        "Let us consider again the estimation problem of Chapter 1. Consider the linear measurement model\n",
        "\n",
        "$$\n",
        "y_i = a_i^\\top x + v_i \\qquad i = 1,\\dots,m \\tag{2}\n",
        "$$\n",
        "\n",
        "where $x\\in\\mathbb{R}^n$ is a vector that we want to estimate, $y_i\\in\\mathbb{R}$ are measured quantities and $v_i$ are measurement errors which are indipendent and identically distributed. Differently from Chapter 1, assume that $v_i$ are variables with density $p$ on $\\mathbb{R}$. The likelihood function is given by\n",
        "\n",
        "$$\n",
        "p_x(y) = \\prod_{i=1}^{m} p(y_i - a_i^\\top x)\n",
        "$$\n",
        "\n",
        "So the ML problem is\n",
        "\n",
        "$$\n",
        "\\max_x  \\quad \\sum_{i=1}^{m} \\log p(y_i - a_i^\\top x)\n",
        "$$\n",
        "\n",
        "If the density $p$ is log-convace (i.e. $\\log p$ is concave) then this problem is convex and can be interpreted as an approximation problem (Chapter 1) with penalty function $\\phi(\\cdot) =  - \\log p (\\cdot)$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s8VTAhBuSchP"
      },
      "source": [
        "Conversely, we can interpret any penalty function approximation problem\n",
        "\n",
        "$$\n",
        "\\displaystyle \\min_x  \\quad \\sum_{i=1}^{m} \\phi(y_i - a_i^\\top x)\n",
        "$$\n",
        "\n",
        "as a maximum likelihood estimation problem with noise density\n",
        "\n",
        "$$\n",
        "p(z) = \\frac{e^{-\\phi(z)}}{\\int e^{-\\phi(u)} du }\n",
        "$$\n",
        "\n",
        "and measurements $b$. This observation gives a statistical interpretation of the penalty function approximation problem. Suppose, for example, that the penalty function $\\phi$ grows very rapidly for large values, which means that we attach a very large cost or penalty to large residuals. The corresponding noise density function $p$ will have very small tails, and the ML estimator will avoid (if possible) estimates\n",
        "with any large residuals because these correspond to very unlikely events.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Example 2.1:** (*Gaussian noise*) When $v_i$ in (2) are [Gaussian](https://colab.research.google.com/drive/1WwNsPbW7-2PyVM_fXsOCUX7O7q8lQM3W#scrollTo=GnMXhs7nXT7M) with zero mean and variance $\\sigma^2$, i.e. they have density $p(z) = (2\\pi \\sigma^2)^{-1/2} e^{-z^2/(2\\sigma^2)}$, then the log-likelihood is\n",
        "\n",
        "$$\n",
        "l(x) = -\\frac{m}{2}\\log(2\\pi \\sigma^2) - \\frac{1}{2\\sigma^2} ||Ax - y||_2^2,\n",
        "$$\n",
        "\n",
        "where $A$ is the matrix with rows $a_1^\\top$, ..., $a_m^\\top$. Therefore the ML solution is the solution of the least-square approximation problem. "
      ],
      "metadata": {
        "id": "NbZiEpdW82sb"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ORn-PBBCzl98"
      },
      "source": [
        "**Example 2.2:** (*Laplacian noise*) When $v_i$ in $(2)$ are [Laplacian](https://colab.research.google.com/drive/1WwNsPbW7-2PyVM_fXsOCUX7O7q8lQM3W#scrollTo=ezGKhrW89az2&line=13&uniqifier=1), i.e. they have density $p(z) = 1/(2a) e^{-|z|/a}$ with $a>0$, then the log-likelihood is\n",
        "\n",
        "$$\n",
        "l(x) = - m \\log(2 a) - \\frac{1}{a} ||Ax - y||_1.\n",
        "$$\n",
        "\n",
        "Therefore the ML solution is the solution of the $\\ell_1$-norm approximation problem.\n",
        "\n",
        "We can understand the robustness of the $\\ell_1$-norm approximation to large errors in terms of maximum likelihood estimation. The Laplacian density has larger tails than the Gaussian, i.e., the probability of a very large $v_i$ is far larger with a Laplacian than with a Gaussian density. As a result, the associated maximum likelihood method expects to see greater numbers of large residuals."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5auUo1oV1cPR"
      },
      "source": [
        "## 2.1.3 Counting with the Poisson distribution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "os7sLBoa1cf4"
      },
      "source": [
        "Counting events is often modelled using a [Poisson distribution](https://colab.research.google.com/drive/1WwNsPbW7-2PyVM_fXsOCUX7O7q8lQM3W#scrollTo=F3PtR1ZsU-RT&line=8&uniqifier=1) with mean $\\mu>0$, namely\n",
        "\n",
        "$$\n",
        "\\textbf{prob}(y = k) = \\frac{e^{-\\mu}\\mu^k}{k!}.\n",
        "$$\n",
        "\n",
        "In a simple statistical model, the mean $\\mu$ is modeled as an affine function of a vector $u \\in \\mathbb{R}^n$\n",
        "\n",
        "$$\n",
        "\\mu = a^\\top u + b.\n",
        "$$\n",
        "\n",
        "Here $u$ is called the vector of *explanatory variables* and $a\\in\\mathbb{R}^n$ and $b\\in \\mathbb{R}$ are the *model parameters*. For instance, consider again the traffic accidents problem and recall that $y$ is the number of traffic accidents in some region over some period. Then the explanatory variables are $u_1$ (the total traffic flow through the region during the period), $u_2$ (the rainfall in the region during the period), and so on. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L2NhAsSk1cis"
      },
      "source": [
        "We are given a number of observations which consist of pairs $(u_i, y_i)$, $i = 1,\\dots,m$. We want to determine the ML estimate of the model parameters $a$ and $b$. The likelihood function has the form\n",
        "\n",
        "$$\n",
        "\\prod_{i=1}^m \\frac{(a^\\top u_i + b)^{y_i} e^{-(a^\\top u_i +b)}}{y_i!}.\n",
        "$$\n",
        "\n",
        "Thus the convex optimisation problem that we want to solve is\n",
        "\n",
        "$$\n",
        "\\max_{a,b}  \\quad \\sum_{i=1}^{m} \\left( y_i \\log (a^\\top u_i + b) - (a^\\top u_i + b)\\right)\n",
        "$$\n",
        "\n",
        "where we dropped $-\\log(y_i!)$ as it is an additive constant (so it will not change the optimal point, i.e. the solution $a^*$, $b^*$)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MofoZMCLfxTH"
      },
      "source": [
        "## 2.1.4 Logistic regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ZEce9G_ScjZ"
      },
      "source": [
        "Let $p$ be the probability of acquiring a certain disease. There are two possible states for an individual, namely $y=1$ means sick while $y=0$ means healthy. Thus, $y$ is a random variable in $\\{0,1\\}$ with \n",
        "$$\n",
        "\\textbf{prob}(y=1) = p, \\qquad \\textbf{prob}(y=0) = 1-p\n",
        "$$\n",
        "where $p \\in [0,1]$. The probability $p$ is modelled as a logistic model \n",
        "\n",
        "$$\n",
        "p = \\frac{e^{a^\\top u + b}}{1+e^{a^\\top u + b}} \\tag{3}\n",
        "$$\n",
        "\n",
        "as a function of some explanatory variables $u$, e.g. weight, age, height, blood pressure, and other medically relevant variables. The model paramters $a\\in\\mathbb{R}^n$ and $b\\in\\mathbb{R}$ determine how the probability $p$ varies as a function of the explanatory variable $u$. We can re-order the data so for $u_1$, ... , $u_q$, the outcome is $y = 1$, and for $u_{q+1}$, ... , $u_m$ the outcome is $y = 0$. The likelihood function then has the form\n",
        "\n",
        "$$\n",
        "\\prod_{i=1}^q p_i \\prod_{i=q+1}^m (1-p_i)\n",
        "$$\n",
        "where $p_i$ is given by $(3)$ evaluated at $u_i$. Finding the ML estimate of $a$ and $b$ is called logistic regression. The log-likelihood function is given by\n",
        "\n",
        "$$\n",
        "\\begin{array}{rl}\n",
        "l(a,b) &= \\sum_{i=1}^q \\log p_i + \\sum_{i=q+1}^m \\log(1-p_i) = \\sum_{i=1}^q \\log \\frac{e^{a^\\top u_i + b}}{1+e^{a^\\top u_i + b}} + \\sum_{i=q+1}^m \\log \\frac{1}{1+e^{a^\\top u_i + b}}\\\\\n",
        "&= \\sum_{i=1}^q (a^\\top u_i + b) - \\sum_{i=1}^m \\log(1+e^{a^\\top u_i + b}).\n",
        "\\end{array}\n",
        "$$\n",
        "\n",
        "Since $l$ is a concave function of $a$ and $b$, (you will learn that) the logistic regression is a convex optimisation problem.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nHQj9n2QhQMn"
      },
      "source": [
        "<div>\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1LvNLKY0ODz0P00Df6yC98jw7s0C0rLeV\" width=\"400\"/>\n",
        "</div>\n",
        "\n",
        "Figure 2.1. *The dots represent $50$ data points. The data suggest that for $u < 5$, the outcome is more likely to be $y = 0$, while for $u > 5$, the outcome is more likely to be $y = 1$. The data\n",
        "also suggest that for $u < 2$ or so, the outcome is very likely to be $y = 0$, and for $u > 8$ or so, the outcome is very likely to be $y = 1$. The curve represents the logistic function found by solving the maximum ML problem. Source: page 355 of $[1]$.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RXqYcpQqhQPA"
      },
      "source": [
        "# 2.2 Optimal Detector Design"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import HTML\n",
        "HTML('<iframe width=\"850\" height=\"480\" src=\"https://www.youtube.com/embed/RRXq99X5Th4\"></iframe>')"
      ],
      "metadata": {
        "id": "vEkAlG6yRY2f",
        "outputId": "400c7028-3d2f-4fd2-8871-fba24b5bf162",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 505
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<iframe width=\"850\" height=\"480\" src=\"https://www.youtube.com/embed/RRXq99X5Th4\"></iframe>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fd4lD5tWvyUs"
      },
      "source": [
        "## 2.2.1 Formulation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nNBqENsyhQRf"
      },
      "source": [
        "Let $x$ be a random variable with values in $\\{1, \\dots, n\\}$ with a distribution that depends on a parameter $\\theta \\in \\{1, \\dots, m\\}$. The distribution of $x$ for the $m$ possible values of $\\theta$ can be described by a matrix $P\\in\\mathbb{R}^{n \\times m}$ of [conditional probabilities](https://colab.research.google.com/drive/1WwNsPbW7-2PyVM_fXsOCUX7O7q8lQM3W#scrollTo=rtFofLWOCFAK&line=5&uniqifier=1) with elements \n",
        "\n",
        "$$\n",
        "p_{kj} = \\textbf{prob}(x = k | \\theta = j).\n",
        "$$\n",
        "\n",
        "Thus the $j$-th column of $P$ gives the probability distribution associated with the parameter value $\\theta = j$. We consider the problem of guessing which distribution has generated an observed sample $x$. The $m$ values of $\\theta$ are called hypotheses and the problem of guessing which hyphotesis is correct is called **hypothesis testing**. If one of the hypothesis correspons to a normal scenario, while the other $m-1$ hypothesis correspond to unlikely scenarios, then hypothesis testing can be interpreted as the problem of guessing if an event is normal or abnormal, and if so detect which unlikely event occured, by looking at the value of a measured variable $x$. In this case the hypothesis testing problem is also called **optimal detection problem**.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UM5tOslj-YpK"
      },
      "source": [
        "A standard parametrization for this problem makes use of a so-called randomised detector $T \\in \\mathbb{R}^{m \\times n}$. A **randomized detector** $T$ is defined element-wise as\n",
        "\n",
        "$$\n",
        "t_{ik} = \\textbf{prob}(\\hat \\theta = i | x = k).\n",
        "$$\n",
        "\n",
        "If we observe $x=k$, then the detector returns the hypothesis $\\hat \\theta = i$ with probability $t_{ik}$. The $k$-th column of $T$, which we denote $t_k$, gives the probability distribution of $\\hat \\theta$, when we observe $x = k$. If each column of $T$ is a [standard unit vector](https://en.wikipedia.org/wiki/Standard_basis), then the randomized detector is called **deterministic detector**. Consider for instance the randomized detector with column $j$ as $[0.8, 0.2]$ and the determnistic detector with column $j$ as $[1, 0]$. If our measurement is $x=j$ then the randomized detector tells us that the event occured is 1 with probability $0.8$ while the determinitic detector tells us that the event occured is 1 with certainty.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wJiD2zw4-YyF"
      },
      "source": [
        "In our optimisation problem $T$ is our optimisation variable. We are interested in designing $T$ that defines the randomised (potentially deterministic) detector. This problems naturally requires the constraints \n",
        "\n",
        "$$\n",
        "t_k \\succcurlyeq 0 \\quad \\mathbf{1}^\\top t_k = 1. \n",
        "$$\n",
        "\n",
        "For the randomized detector defined by the matrix $T$, we define the **detection probability matrix** as $D = TP$. Thus the elements of $D$ are\n",
        "\n",
        "$$\n",
        "d_{ij} = (TP)_{ij} = \\textbf{prob}(\\hat \\theta = i | \\theta = j).\n",
        "$$\n",
        "\n",
        "So $d_{ij}$ is the probability of guessing $\\hat \\theta = i$ when in fact $\\theta = j$. This matrix is also called **confusion matrix** in some fields, especially when it is $2$ by $2$. The diagonal elements of $D$, called **detection probabilities**, indicate the probability of true positives, i.e. probability of guessing $\\hat \\theta = i$ when $\\theta = i$. The off-diagonal elements of $D$, called **error probabilities**, indicate the probability of false positives, the probability of returning $\\hat \\theta = i$ when it is $\\theta = j$. If $D$ is the identity then the detector is **perfect**.  \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can also easily impose lower bounds on the probability of correctly detecting the $j$-th hipothesis by simply requiring\n",
        "\n",
        "$$\n",
        "d_{jj} \\ge L_j,\n",
        "$$\n",
        "\n",
        "where $L_j$ is a threshold.\n",
        "\n"
      ],
      "metadata": {
        "id": "aMsZWfC9GDpO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "One can formulate several different optimal detector design problems. Here we consider two instances of a multi-objective optimisation problem. Consider $m(m-1)$ objectives given by the off-diagonal entries of $D$, which are the probabilities of different types of detection error\n",
        "\n",
        "$$\n",
        "\\begin{array}{ll}\n",
        "\\displaystyle \\min_T & D_{ij} \\qquad i,j = 1,\\dots,m, \\quad i\\ne j\\\\\n",
        "s.t. &t_k \\succcurlyeq 0, \\quad \\mathbf{1}^\\top t_k = 1, \\qquad k = 1, \\dots, n.\n",
        "\\end{array}\n",
        "$$\n",
        "\n",
        "Recall that solving a multi-objective optimisation problem consists in determining the curve of Pareto optimal points. To this end we look at two scalarisation approaches: minimax and weighted sum."
      ],
      "metadata": {
        "id": "a5-67nftGZyE"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mmPJqY_O-Y7i"
      },
      "source": [
        "\n",
        "\n",
        "The first approach that we take is the **minimax detector design problem**. Here we want to minimize the maximum error probability, i.e. $\\max_j (1-D_{jj})$. So the problem can be formulated as\n",
        "\n",
        "$$\n",
        "\\begin{array}{ll}\n",
        "\\displaystyle \\min_T &\\max_j 1-D_{jj}\\\\\n",
        "s.t. &t_k \\succcurlyeq 0, \\quad \\mathbf{1}^\\top t_k = 1, \\qquad k = 1, \\dots, n.\n",
        "\\end{array}\\tag{4}\n",
        "$$\n",
        "\n",
        "The minimax detector minimizes the worst-case (largest) probability of error over all $m$\n",
        "hypotheses.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yju7mJsk_6EZ"
      },
      "source": [
        "Another common approach involves the use of a **weighted sum**. We can scalarize the multiobjective problem by considering a weighted sum of the objectives\n",
        "\n",
        "$$\n",
        "\\sum_{i,j=1}^m W_{ij}D_{ij} = \\textbf{tr}(W^\\top D) = \\textbf{tr}(PW^\\top T) = \\sum_{k=1}^n c_{k}^\\top t_{k} \\tag{5}\n",
        "$$\n",
        "where $c_k$ is the $k$-th column of $WP^\\top$ and $W$ is a matrix of weights. As usual, by solving the optimisation problems by spanning the values of the wieghts $W$ we find multiple points belonging to the Pareto optimal trade-off curve. Note that the parametrization $(5)$ is particularly nice because the constraints are separable, i.e. we have separate constraints on each $t_k$. Therefore, for each weight matrix $W$ one can solve $n$ simple linear programs\n",
        "\n",
        "$$\n",
        "\\begin{array}{ll}\n",
        "\\displaystyle \\min_{t_k} & c_k^\\top t_k\\\\\n",
        "s.t. &t_k \\succcurlyeq 0, \\quad \\mathbf{1}^\\top t_k = 1.\n",
        "\\end{array}\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We explore an application of the formulation presented above in the next section."
      ],
      "metadata": {
        "id": "8KObLzpzJ28Z"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fcwnMy_qv5Wr"
      },
      "source": [
        "### Binary hypothesis testing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t6Enr_JEhQVP"
      },
      "source": [
        "Consider the special case $m=2$ which is called binary hypothesis testing. The random variable $x$ is generated from one of two distributions,\n",
        "which we denote $p \\in \\mathbb{R}^n$ and $q \\in \\mathbb{R}^n$. Usually this corresponds to scenarios in which hypothesis 1 is a normal situation while hypothesis 2 is an abnormal situation that we are trying to detect. Then if $\\hat \\theta = 1$ we say that the test for the abnormal scenario is negative, while if $\\hat \\theta = 2$ we say that the test for the abnormal scenario is positive. The detection probability matrix is usually expressed as\n",
        "\n",
        "$$\n",
        "D = \\left[\\begin{array}{cc}Tp & Tq\\end{array}\\right] = \\left[\\begin{array}{cc}1-P_{fp} & P_{fn}\\\\ P_{fp} & 1-P_{fn}\\end{array}\\right]\n",
        "$$\n",
        "where $P_{fn}$ is the probability of a false negative and $P_{fp}$ is the probability of a false positive. The Pareto optimal trade-off curve is called *receiver operating characterestic* (ROC). The curve depends only on the ratio of the weights, namely $\\lambda = W_{12}/W_{21}$. Thus the weighted sum scalarisation of the multiobjective problem can be formulated as\n",
        "\n",
        "$$\n",
        "\\begin{array}{ll}\n",
        "\\displaystyle \\min_T & (Tp)_2 + \\lambda (Tq)_1 = P_{fp}+\\lambda P_{fn}\\\\\n",
        "s.t. &t_{1k} + t_{2k} = 1, \\quad t_{ik}\\ge 0, \\quad i=1,2, \\quad k = 1, \\dots, n\n",
        "\\end{array}\\tag{6}\n",
        "$$\n",
        "\n",
        "It can be shown that the solution is\n",
        "\n",
        "$$\n",
        "(t_{1k},t_{2k}) = \\left\\{ \\begin{array}{l}(1,0) \\qquad p_k \\ge \\lambda q_k\\\\ (0,1) \\qquad p_k < \\lambda q_k  \\end{array}\\right.\n",
        "$$\n",
        "\n",
        "which is a determinitic detector which is called *likelihood ratio threshold test*. By choosing different values of the threshold,\n",
        "we obtain (deterministic) Pareto optimal detectors that give different levels of\n",
        "false positive versus false negative error probabilities. The likelihood ratio detectors do not give all the Pareto optimal detectors; they are just the vertices of the optimal trade-off curve, which is piecewise-linear. This is shown in the example below.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7sqheDfFwAXb"
      },
      "source": [
        "**Example 2.3:** For instance consider problem $(6)$ with \n",
        "\n",
        "$$\n",
        "P = \\left[\\begin{array}{cc}0.70 & 0.10\\\\ 0.20 & 0.10 \\\\ 0.05 & 0.70 \\\\ 0.05 & 0.10\\end{array}\\right].\n",
        "$$\n",
        "\n",
        "The code that solves this problem and determines the optimal tradeoff curve is provided at the end of this example for the interested student. The optimal trade-off curve is shown below.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nm9z5SKuhQXX"
      },
      "source": [
        "<div>\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1sNiDqbitSCZqoc6LeQ0Iu6co_bWod2oU\" width=\"500\"/>\n",
        "</div>\n",
        "\n",
        "\n",
        "Figure 2.2. *Optimal trade-off curve between probability of a false negative ($P_{fn}$), and probability of a false positive ($P_{fp}$). The vertices of the trade-off curve, labeled 1, 2, 3, N and P, correspond to deterministic\n",
        "detectors; the point labeled 4, which is a randomized detector, is the minimax\n",
        "detector.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zUyoHzU8hQZ0"
      },
      "source": [
        "We highlight 6 points. The left endpoint, labeled N, corresponds to the deterministic detector which is always negative, independently of the observed value of $x$; the right endpoint, labeled P, corresponds to the deterministic detector that is always positive. The vertices labelled $1$, $2$ and $3$ corresponds to the deterministic detectors\n",
        "\n",
        "$$\n",
        "T^{(1)}=\\left[\\begin{array}{cc}1 & 1 & 0 & 1\\\\ 0 & 0 & 1 & 0\\end{array}\\right],\\quad T^{(2)}=\\left[\\begin{array}{cc}1 & 1 & 0 & 0\\\\ 0 & 0 & 1 & 1\\end{array}\\right], \\quad T^{(3)}=\\left[\\begin{array}{cc}1 & 0 & 0 & 0\\\\ 0 & 1 & 1 & 1\\end{array}\\right].\n",
        "$$\n",
        "\n",
        "In fact, note that for instance $T^{(1)}P=\\left[\\begin{array}{cc}0.95 & 0.30\\\\ 0.05 & 0.70\\end{array}\\right]$ which corresponds to the point $(P_{fp},P_{fn})=(0.05,0.3)$ in the figure. These 5 detectors are obeined by solving Problem $(6)$. However, these detectors corresponpond only to the corner points of the piece-wise Pareto optimal curve. Thus, the remaining points cannot be obtained by scalarisation via weighted sum. Finally, point $4$ corresponds to the minimax detector obtained by solving Problem $(4)$\n",
        "\n",
        "$$\n",
        "T^{(4)}=\\left[\\begin{array}{cc}1 & 2/3 & 0 & 0\\\\ 0 & 1/3 & 1 & 1\\end{array}\\right]\n",
        "$$\n",
        "\n",
        "which yields equal probability of false positive and false negative, in this case $(P_{fp},P_{fn})=(1/6,1/6)$. No deterministic detector can achieve this Pareto optimal point."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Code for the example above"
      ],
      "metadata": {
        "id": "blvwmsIlpR9t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We import the libraries\n",
        "import cvxpy as cp\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "XB5BdVYRpNA_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the given matrix P\n",
        "P = np.array([[0.7,0.1],[0.2,0.1],[0.05,0.7],[0.05,0.1]])\n",
        "n=4; m=2;"
      ],
      "metadata": {
        "id": "K4qFLhiqpRLO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Declate the variables, define D and the constraints\n",
        "T = cp.Variable((m,n))\n",
        "D=T@P\n",
        "constr = [cp.sum(T,0)==1, T>=0]"
      ],
      "metadata": {
        "id": "3HK0AvsmpXud"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Declare values for lambda between 0 and 10\n",
        "num_l=100\n",
        "L=np.linspace(10, 0, num=num_l)\n",
        "# Initialise variables\n",
        "opt_val = np.zeros(num_l)\n",
        "x = np.zeros(num_l)\n",
        "y = np.zeros(num_l)\n",
        "init=np.array([[0., 0., 0., 0.],\n",
        "       [0., 0., 0., 0.]])\n",
        "storeT = [init]\n",
        "k=0\n",
        "\n",
        "# Solve the scalarized problem\n",
        "for i in range(num_l):\n",
        "  obj = D[1][0]+L[i]*D[0,1]\n",
        "  prob = cp.Problem(cp.Minimize(obj),constr)\n",
        "  prob.solve()\n",
        "  Dtmp = T.value@P\n",
        "  x[i]=1-Dtmp[0][0]\n",
        "  y[i]=1-Dtmp[1][1]\n",
        "  # Whenever we find a new detector, we store it.\n",
        "  if (not np.array_equal(storeT[k],np.round(T.value,2))):\n",
        "    storeT = storeT + [np.round(T.value,2)]\n",
        "    k=k+1\n",
        "\n",
        "# It turns out that we found 5 detectors.\n",
        "print(\"Scalarized detectors:\")\n",
        "print(storeT[1])\n",
        "print(storeT[2])\n",
        "print(storeT[3])\n",
        "print(storeT[4])\n",
        "print(storeT[5])\n",
        "\n",
        "# This shows that here scalarization can find only 5 points on the Pareto \n",
        "# optimal curve"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LWexw_dapfTp",
        "outputId": "d3e8b22f-214c-4a32-dbb9-2b4a006493e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scalarized detectors:\n",
            "[[0. 0. 0. 0.]\n",
            " [1. 1. 1. 1.]]\n",
            "[[1. 0. 0. 0.]\n",
            " [0. 1. 1. 1.]]\n",
            "[[1. 1. 0. 0.]\n",
            " [0. 0. 1. 1.]]\n",
            "[[1. 1. 0. 1.]\n",
            " [0. 0. 1. 0.]]\n",
            "[[1. 1. 1. 1.]\n",
            " [0. 0. 0. 0.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Declare the objective value of the minimax detector problem\n",
        "# Solve the minimax problem and find T4\n",
        "obj = cp.maximum(D[0][1],D[1][0])\n",
        "prob = cp.Problem(cp.Minimize(obj),constr)\n",
        "prob.solve()\n",
        "MinMaxT=np.round(T.value,2)\n",
        "print(\"Minmax detector:\")\n",
        "print(MinMaxT)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1qu7NLs9mTgK",
        "outputId": "f593d1dc-93eb-4030-c82a-7850b1faddc3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Minmax detector:\n",
            "[[1.   0.67 0.   0.  ]\n",
            " [0.   0.33 1.   1.  ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# We generate the Figure 2.2\n",
        "plt.plot(x,y)\n",
        "plt.xlim((0,1))\n",
        "plt.ylim((0,1))\n",
        "plt.xlabel(\"$P_{fp}$\",fontsize=16)\n",
        "plt.ylabel(\"$P_{fn}$\",fontsize=16)\n",
        "plt.text((storeT[5]@P)[1][0],(storeT[5]@P)[0][1],'N')\n",
        "plt.text((storeT[4]@P)[1][0],(storeT[4]@P)[0][1],'$1$')\n",
        "plt.text((storeT[3]@P)[1][0],(storeT[3]@P)[0][1],'$2$')\n",
        "plt.text((storeT[2]@P)[1][0],(storeT[2]@P)[0][1],'$3$')\n",
        "plt.text((MinMaxT@P)[1][0],(MinMaxT@P)[0][1],'$4$')\n",
        "plt.text((storeT[1]@P)[1][0],(storeT[1]@P)[0][1],'P')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 298
        },
        "id": "X2Lptu8FpjSk",
        "outputId": "f34b013f-4fcb-4cbe-de58-8c6908470104"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vhYLTUkEVuJe"
      },
      "source": [
        "# End of CHAPTER 2\n"
      ]
    }
  ]
}