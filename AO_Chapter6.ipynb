{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SV-jbTxB5N2U"
      },
      "source": [
        "# CHAPTER 6 - Duality"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "---\n",
        "\n",
        "**Author:** Dr Giordano Scarciotti (g.scarciotti@imperial.ac.uk) - Imperial College London \n",
        "\n",
        "**Module:** ELEC70066 - Advanced Optimisation\n",
        "\n",
        "**Version:** 1.1.4 - 22/02/2023\n",
        "\n",
        "---\n",
        "---"
      ],
      "metadata": {
        "id": "IAva7-95xliR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The material of this chapter is adapted from $[1]$."
      ],
      "metadata": {
        "id": "XXbAto9OxowK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this chapter we define duality. Thanks to duality we define necessary and sufficient conditions of optimality, provide lower bounds of the optimal value and a sensitivity analysis with respect to the violation of the constraints. Contents:\n",
        "\n",
        "*   Section 6.1 The Lagrange Dual Function\n",
        "*   Section 6.2 Lagrange Dual Problem\n",
        "*   Section 6.3 Geometric Interpretation\n",
        "*   Section 6.4 Optimality Conditions\n",
        "*   Section 6.5 Perturbation and Sensitivity Analysis\n",
        "*   Section 6.6 Duality and Problem Reformulations\n",
        "*   Section 6.7 Generalised Inequalities"
      ],
      "metadata": {
        "id": "ljCtDG3Dxtz1"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WXPDiqZC5N5Z"
      },
      "source": [
        "# 6.1 The Lagrange Dual Function"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6.1.1 Definitions"
      ],
      "metadata": {
        "id": "5AZKfHVRnPdi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import HTML\n",
        "HTML('<iframe width=\"850\" height=\"480\" src=\"https://www.youtube.com/embed/DBFv3pffBUk\"></iframe>')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 505
        },
        "id": "fesLLdvJyq2e",
        "outputId": "438277a5-c882-4808-ad74-54e177c50016"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<iframe width=\"850\" height=\"480\" src=\"https://www.youtube.com/embed/DBFv3pffBUk\"></iframe>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pQfIR7po5N8N"
      },
      "source": [
        "Consider a general (i.e. may or may not be convex) optimisation problem \n",
        "\n",
        "$$\n",
        "\\begin{array}{lll}\n",
        "\\min & f_0(x) &\\\\\n",
        "s.t. & f_i(x) \\le 0, & i = 1,\\dots,m\\\\\n",
        "& h_i(x) = 0,  & i = 1,\\dots,p,\n",
        "\\end{array} \\tag{1}\n",
        "$$\n",
        "\n",
        "assume that its domain $\\mathcal{D}$ is non-empty and denote the optimal value by $p^*$. Define the **Lagrangian** as\n",
        "\n",
        "$$\n",
        "\\displaystyle L(x,\\lambda, \\nu) = f_0(x) + \\sum_{i=1}^m \\lambda_i f_i(x) + \\sum_{i=1}^p \\nu_i h_i(x)\n",
        "$$\n",
        "\n",
        "with domain $\\textbf{dom }L = \\mathcal{D} \\times \\mathbb{R}^m\\times \\mathbb{R}^p$. The vectors $\\lambda$ and $\\nu$ are called **dual variables** or **Lagrangian multiplier vectors** associated to problem $(1)$. We define the **Lagrange dual function**, or simply **dual function**, as the minimum value of the Lagrangian over $x$, namely\n",
        "\n",
        "$$\n",
        "\\displaystyle  g(\\lambda, \\nu) = \\inf_{x\\in\\mathcal{D}}  L(x,\\lambda, \\nu) = \\inf_{x\\in\\mathcal{D}} \\left( f_0(x) + \\sum_{i=1}^m \\lambda_i f_i(x) + \\sum_{i=1}^p \\nu_i h_i(x)\\right)\n",
        "$$\n",
        "\n",
        "Since the dual function is the pointwise infimum of a family of affine functions in $\\lambda$ and $\\nu$, it is concave (always, no matter whether $(1)$ is convex or not).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lc-Mw_Hz6Nug"
      },
      "source": [
        "The dual function yields lower bounds on the optimal value $p^*$ of problem $(1)$. In fact, suppose that $\\tilde x$ is a feasible point for problem $(1)$ and $\\lambda \\succcurlyeq 0$. Then\n",
        "\n",
        "$$\n",
        "\\sum_{i=1}^m \\lambda_i f_i(\\tilde x) + \\sum_{i=1}^p \\nu_i h_i(\\tilde x) \\le 0\n",
        "$$\n",
        "\n",
        "since the equality constraints are satisfied (so they sum to zero) and the inequality constraints are satisfied (so they sum to a non-positive number). Thus\n",
        "\n",
        "$$\n",
        "L(\\tilde x,\\lambda, \\nu) \\le f_0(\\tilde x)\n",
        "$$\n",
        "\n",
        "But since $g$ is the infimum of $L$, we have\n",
        "\n",
        "$$\n",
        "g(\\lambda, \\nu)= \\inf_{x \\in \\mathcal{D}} L(x,\\lambda, \\nu) \\le L(\\tilde x,\\lambda, \\nu) \\le f_0(\\tilde x).\n",
        "$$\n",
        "\n",
        "Since this holds for any feasible $\\tilde x$, it must hold for the optimal point. So\n",
        "\n",
        "$$\n",
        "g(\\lambda, \\nu) \\le p^*\n",
        "$$\n",
        "\n",
        "When the Lagrangian is unbounded, the lower bound is $-\\infty$, which is not very informative. The lower bound is meaningful only when $\\lambda \\succcurlyeq 0$ and $(\\lambda, \\nu)\\in\\textbf{dom }g$, that is when $g(\\lambda,\\nu)> -\\infty$. In this case, we say that $(\\lambda,\\nu)$ is a **dual feasible** pair. The reason of this terminology will be clear later.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kNtCXzoJ6NxB"
      },
      "source": [
        "## 6.1.2 Examples"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import HTML\n",
        "HTML('<iframe width=\"850\" height=\"480\" src=\"https://www.youtube.com/embed/fYfpv1vRfQU\"></iframe>')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 505
        },
        "id": "LcYO8J_Vyvgk",
        "outputId": "ff5994bf-fa9a-40c6-e36d-263ce5e44e7e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<iframe width=\"850\" height=\"480\" src=\"https://www.youtube.com/embed/fYfpv1vRfQU\"></iframe>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lZQN0BIu6Nzo"
      },
      "source": [
        "We look now at a few simple examples.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bh5O7zKOBTS7"
      },
      "source": [
        "### Least-squares solution of linear equations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6VrfP2-0BTVW"
      },
      "source": [
        "Consider the problem\n",
        "\n",
        "$$\n",
        "\\begin{array}{ll}\n",
        "\\min & x^\\top x \\\\\n",
        "s.t. & Ax=b,  \n",
        "\\end{array}\n",
        "$$\n",
        "\n",
        "where $A\\in\\mathbb{R}^{p \\times n}$. The Lagrangian is $L(\\lambda,\\nu) = x^\\top x + \\nu^\\top (Ax-b)$. Since this is a convex quadratic function in $x$, we compute the gradient and set it to zero, $\\nabla_x L(\\lambda,\\nu) = 2x + A^\\top \\nu =0$, from which it follows that $x = -(1/2)A^\\top \\nu$ . Hence,\n",
        "\n",
        "$$\n",
        "g(\\nu) = -\\frac{1}{4} \\nu^\\top AA^\\top \\nu - b^\\top \\nu,\n",
        "$$\n",
        "\n",
        "which is a concave quadratic function. So for any $\\nu$ we have\n",
        "\n",
        "$$\n",
        "-\\frac{1}{4} \\nu^\\top AA^\\top \\nu - b^\\top \\nu \\le \\inf\\{x^\\top x : Ax=b \\}.\n",
        "$$\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l3Ox0pg8BTX4"
      },
      "source": [
        "### Standard form LP"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pxdcaJgEBTaX"
      },
      "source": [
        "Consider the LP in standard form\n",
        "\n",
        "$$\n",
        "\\begin{array}{ll}\n",
        "\\min & c^\\top x \\\\\n",
        "s.t. & Ax=b,\\\\\n",
        "& x \\succcurlyeq 0  \n",
        "\\end{array} \\tag{2}\n",
        "$$\n",
        "\n",
        "The Lagrangian is $c^\\top x + \\nu^\\top (Ax-b) - \\lambda^\\top x = -b \\nu^\\top + (c + A^\\top \\nu - \\lambda)^\\top x$. Now, since $L$ is affine we have an unbounded infimum, unless the linear term disappears. Hence\n",
        "\n",
        "$$\n",
        "g(\\lambda,\\nu) = \\left\\{\\begin{array}{ll}-b^\\top\\nu & A^\\top \\nu - \\lambda +c =0 \\\\ -\\infty & \\text{otherwise.}  \\end{array}\\right. \\tag{3}\n",
        "$$\n",
        "\n",
        "Hence, in this case the lower bound is non-trivial, and equal to $-b^\\top\\nu$, only when $(\\lambda,\\nu)$ satisfy $\\lambda \\succcurlyeq 0$ and $A^\\top \\nu - \\lambda +c =0$. We will see that this type of result is fairly common.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kpPBFvxX6N2P"
      },
      "source": [
        "### Two-way partitioning problem"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3L2Tph_a6N4h"
      },
      "source": [
        "Consider the non-convex problem\n",
        "\n",
        "$$\n",
        "\\begin{array}{lll}\n",
        "\\min & x^\\top W x & \\\\\n",
        "s.t. & x_i^2=1, & i=1,\\dots,n \n",
        "\\end{array}\n",
        "$$\n",
        "\n",
        "where $W \\in \\mathbb{S}^n_+$. This problem consists in partitioning the elements $\\{1,\\dots,n\\}$ in two sets, according to the cost. The matrix coefficient $W_{ij}$ represents the cost of assigning $i$ and $j$ to the same set (both $1$ or $-1$), while $-W_{ij}$ represents the cost of assigning $i$ and $j$ to different sets ($1$ and the other $-1$). The number of assignments is finite, but it grows exponentially with $2^n$, so a solution by enumeration is to be excluded for large $n$.  The Lagrangian is\n",
        "\n",
        "$$\n",
        "L(x,\\nu) = x^\\top W x + \\sum_{i-1}^n \\nu_i(x_i^2-1) = x^\\top (W + \\textbf{diag}(\\nu))x - \\mathbf{1}^\\top \\nu.\n",
        "$$\n",
        "\n",
        "This is a quadratic function. If the quadratic term is positive semidefinite, then we want it to be zero. If the quadratic term has a negative eigenvalue, then we can make the cost unbounded. In summary\n",
        "\n",
        "$$\n",
        "g(\\lambda,\\nu) = \\left\\{\\begin{array}{ll}-\\mathbf{1}^\\top\\nu & W + \\textbf{diag}(\\nu) \\succcurlyeq 0 \\\\ -\\infty & \\text{otherwise.}  \\end{array}\\right.\n",
        "$$\n",
        "\n",
        "This dual function provides lower bounds on the optimal value of a difficult problem. For instance, if $\\nu = - \\lambda_{\\min}(W) \\mathbf{1}$, then $p^* \\ge -\\mathbf{1}^\\top\\nu = n \\lambda_{\\min}(W)$, where $\\lambda_{\\min}(W)$ is the minimum eigenvalue of $W$.\n",
        "\n",
        "Note that this lower bound can be obtained also in another way. The constraints $x_1^2=1$, ..., $x_n^2=1$ imply (but are not implied by) that $\\sum_{i=1}^n x_i^2 = n$. If we relax the problem by considering this last constraint, then the modified problem is simply an eigenvalue problem which has solution $n\\lambda_{\\min}(W)$.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YalVNL8W5N-g"
      },
      "source": [
        "## 6.1.3 Conjugate Functions"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import HTML\n",
        "HTML('<iframe width=\"850\" height=\"480\" src=\"https://www.youtube.com/embed/J8slAaQTuwg\"></iframe>')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 505
        },
        "id": "5PFiNGufyx51",
        "outputId": "19319f13-9d21-47b6-84ff-ff602c0e6ec1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<iframe width=\"850\" height=\"480\" src=\"https://www.youtube.com/embed/J8slAaQTuwg\"></iframe>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_D8PhdfAJtzu"
      },
      "source": [
        "The Lagrangian dual function is closely related to the so-called conjugate function.\n",
        "\n",
        "Let $f : \\mathbb{R}^n \\to \\mathbb{R}$. The function $f^∗ : \\mathbb{R}^n \\to \\mathbb{R}^n$, defined as \n",
        "\n",
        "$$\n",
        "f^∗(y) = \\sup_{x\\in\\textbf{dom }f}\\left(y^\\top x - f(x)  \\right),\n",
        "$$\n",
        "\n",
        "\u0001is called the **conjugate** of the function $f$. The domain of the conjugate function consists of all $y$ for which the supremum is finite, i.e., for which the difference $y^\\top x - f(x)$ is bounded above on $\\textbf{dom }f$. This definition is illustrated in the figure below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZFLqAiXQU4OP"
      },
      "source": [
        "<div>\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1cUpzRrxFzbMbWbXsFVWp4Ohf1gCYJnGM\" width=\"400\"/>\n",
        "</div>\n",
        "\n",
        "Figure 6.1. *The conjugate function $f^*(y)$ is the maximum gap between the linear function $yx$ and $f(x)$. Source: page $91$ of $[1]$.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YdaZp86SJt2c"
      },
      "source": [
        "Obviously $f^∗$ is a convex function, since it is the pointwise\n",
        "supremum of a family of affine functions of $y$. This is true whether\n",
        "or not $f$ is convex. We now look at some examples."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R8hpcLAwJt4_"
      },
      "source": [
        "*   *Affine function*: $f(x) = ax + b$. As a function of $x$, $yx − ax − b$ is bounded if and only if $y = a$, in which case it is constant. Therefore the domain of the conjugate function $f^∗$ is the singleton $\\{a\\}$, and $f^∗(a) = −b$.\n",
        "*    *Negative logarithm*: $f(x) = −\\log x$, with $\\textbf{dom }f = \\mathbb{R}_{++}$. The function $xy+\\log x$ is unbounded above if $y \\ge 0$. Computing the gradient, the maximum is reached at $x = −1/y$. Therefore, $\\textbf{dom }f^∗ = \\{y : y < 0\\} = −\\mathbb{R}_{++}$ and $f^∗(y) = −\\log(−y)−1$ for $y < 0$.\n",
        "*    *Log-determinant*: $f(X) = \\log \\det X^{−1}$ on $\\mathbb{S}^n_{++}$. We skip the proof, but recall that $\\log \\det X^{−1}$ on $\\mathbb{S}^n_{++}$ plays the role of $−\\log x$ on $\\mathbb{R}^n_{++}$. In fact the conjugate is $f^*(Y) = \\log \\det (-Y)^{-1}-n$ with $\\textbf{dom }f^* = -\\mathbb{S}^n_{++}$.\n",
        "*    *Exponential*: $f(x) = e^x$. The function $xy − e^x$ is unbounded if $y < 0$. For $y > 0$, $xy − e^x$ reaches its maximum at $x = \\log y$, so we have $f^∗(y) = y \\log y − y$. For $y = 0$, $f^∗(y) = \\sup_x −e^x = 0$.\n",
        "*    *Negative entropy*: $f(x) = x \\log x$, with $\\textbf{dom }f = \\mathbb{R}_{+}$ (and $f(0) = 0$). The function $xy − x \\log x$ is bounded above on $\\mathbb{R}^n_{+}$ for all $y$, hence $\\textbf{dom }f^∗ = \\mathbb{R}$. It attains its maximum at $x = e^{y−1}$, and substituting we find $f^∗(y) = e^{y−1}$.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V_ngajCSbbbm"
      },
      "source": [
        "---\n",
        "\n",
        "**Exercise 6.1:**  Show that the conjugate of the *Inverse* function $f(x) = 1/x$ on $\\mathbb{R}_{++}$ is $f^∗(y) = −2(−y)^\\frac{1}{2}$, with $\\textbf{dom }f^* = -\\mathbb{R}_{+}$.\n",
        "\n",
        "***EDIT THE FILE TO ADD YOUR PROOF HERE***\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t3uE8OmkJt7J"
      },
      "source": [
        "*    *Strictly convex quadratic function*: $f(x) = \\frac{1}{2} x^\\top Qx$, with $Q \\in \\mathbb{S}^n_{++}$. The function $y^\\top x − \\frac{1}{2}x^\\top Qx$ is bounded above as a function of $x$ for all $y$. It attains its maximum at $x = Q^{−1}y$, so $f^∗(y) = \\frac{1}{2}y^\\top Q^{−1}y$.\n",
        "*    *Log-sum-exp*: $f(x) = \\log(\\sum^n_{i=1} e^{x_i})$ has conjugate\n",
        "$$\n",
        "f^*(y) = \\left\\{\\begin{array}{ll} \\sum_{i=1}^n y_i \\log y_i & \\text{if }y\\succcurlyeq 0 \\text{ and } \\mathbf{1}^\\top y =1 \\\\ \\infty & \\text{otherwise.}  \\end{array} \\right.\n",
        "$$\n",
        "The proof is omitted.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jSVx0kloJt9T"
      },
      "source": [
        "---\n",
        "\n",
        "**Exercise 6.2:** Show that the conjugate of the *norm* $f(x)=||x||$, where $||\\cdot||$ is a norm on $\\mathbb{R}^n$ with dual norm $||\\cdot||_*$, is given by\n",
        "\n",
        "$$\n",
        "f^*(y) = \\left\\{\\begin{array}{ll} 0 & ||y||_*\\le 1 \\\\ \\infty & \\text{otherwise.}  \\end{array} \\right.\n",
        "$$\n",
        "\n",
        "***EDIT THE FILE TO ADD YOUR PROOF HERE***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Mpzq7nEJuAB"
      },
      "source": [
        "**Exercise 6.3:** Show that the conjugate of the *squared norm* $f(x) = \\frac{1}{2}||x||^2$ is $f^*(y) = \\frac{1}{2}||y||_*^2$.\n",
        "\n",
        "***EDIT THE FILE TO ADD YOUR PROOF HERE***\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cjIUmZTLgVem"
      },
      "source": [
        "Going back to duality, consider an optimisation problem with linear inequality and equality constraints\n",
        "\n",
        "$$\n",
        "\\begin{array}{ll}\n",
        "\\min & f_0(x) \\\\\n",
        "s.t. & Ax \\preccurlyeq b\\\\\n",
        "& Cx = d. \n",
        "\\end{array} \n",
        "$$\n",
        "\n",
        "The dual function is given by\n",
        "\n",
        "$$\n",
        "\\begin{array}{rl}\n",
        "g(\\lambda,\\nu) &= \\displaystyle\\inf_x (f_0(x) + \\lambda^\\top (Ax-b) + \\nu^\\top (Cx-d)) \\\\\n",
        "&= -b^\\top \\lambda - d^\\top \\nu + \\displaystyle\\inf_x (f_0(x) + (A^\\top \\lambda +C^\\top\\nu)^\\top x) \\\\\n",
        "&= -b^\\top \\lambda - d^\\top \\nu - f_0^*(-A^\\top \\lambda -C^\\top\\nu),\n",
        "\\end{array}\n",
        "$$\n",
        "\n",
        "with domain $\\textbf{dom }g = \\{(\\lambda,\\nu) : -A^\\top \\lambda -C^\\top\\nu \\in \\textbf{dom }f_0^*\\}$. This is a useful result that can be applied to several classes of problems, as shown below.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UttkiydtgVhU"
      },
      "source": [
        "### Equality constrained norm minimization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kfCIIq5ngVj1"
      },
      "source": [
        "Consider the problem\n",
        "\n",
        "$$\n",
        "\\begin{array}{ll}\n",
        "\\min & ||x|| \\\\\n",
        "s.t. & Ax = b \n",
        "\\end{array} \n",
        "$$\n",
        "where $||\\cdot||$ is any norm. Then from the results above we have\n",
        "$$\n",
        "g(\\nu) = -b^\\top \\nu - f_0^*(-A^\\top \\nu) = \\left\\{\\begin{array}{ll} -b^\\top \\nu & ||A^\\top \\nu||_*\\le 1 \\\\ \\infty & \\text{otherwise.}  \\end{array} \\right.\n",
        "$$\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-kr8S7t5gVmV"
      },
      "source": [
        "### Entropy maximization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z1jWKIzogVox"
      },
      "source": [
        "Consider the entropy maximization problem\n",
        "\n",
        "$$\n",
        "\\begin{array}{ll}\n",
        "\\min & \\sum_{i=1}^n x_i \\log(x_i)\\\\\n",
        "s.t. & Ax\\preccurlyeq b\\\\\n",
        "& \\mathbf{1}^\\top x = 1.   \n",
        "\\end{array} \\tag{4}\n",
        "$$\n",
        "\n",
        "From the results above we have\n",
        "\n",
        "$$\n",
        "g(\\lambda,\\nu) = -b^\\top \\lambda - \\nu - \\sum_{i=1}^n e^{-a_i^\\top \\lambda - \\nu -1} = -b^\\top \\lambda - \\nu - e^{-\\nu-1} \\sum_{i=1}^n e^{-a_i^\\top \\lambda}\n",
        "$$\n",
        "\n",
        "where $a_i$ is the $i$-th column of $A$.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p38H54HkxD2K"
      },
      "source": [
        "# 6.2 Lagrange Dual Problem"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import HTML\n",
        "HTML('<iframe width=\"850\" height=\"480\" src=\"https://www.youtube.com/embed/k40FB9sLtYw\"></iframe>')"
      ],
      "metadata": {
        "id": "JzQoJDtoyzmT",
        "outputId": "acccfe48-fd3a-4921-ac29-d7437eae67fb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 504
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<iframe width=\"850\" height=\"480\" src=\"https://www.youtube.com/embed/k40FB9sLtYw\"></iframe>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l6NbKLG9xD5I"
      },
      "source": [
        "Since for any $(\\lambda,\\nu)$ with $\\lambda\\succcurlyeq 0$, the Lagrange dual function provides us with a lower bound for $p^*$, it is natural to seek the best (maximum) lower bound. This leads to the optimisation problem\n",
        "\n",
        "$$\n",
        "\\begin{array}{ll}\n",
        "\\max & g(\\lambda, \\nu) \\\\\n",
        "s.t. & \\lambda \\succcurlyeq 0 \n",
        "\\end{array} \\tag{5}\n",
        "$$\n",
        "\n",
        "called **Lagrange dual problem** associated to **primal problem** $(1)$. The term **dual feasible** that we have already used before to describe the pairs $(\\lambda,\\nu)$ with $\\lambda \\succcurlyeq 0$ and $g(\\lambda,\\nu) > -\\infty$ corresponds to the feasibility of the dual problem $(5)$. We call $(\\lambda^*,\\nu^*)$ **dual optimal** or **optimal Lagrange multipliers** if they are optimal for the dual problem $(5)$.\n",
        "\n",
        "Note that the dual problem $(5)$ is always convex (whether the original problem is convex or not) bacause the objective is the maximisation of a concave function and the constraint is convex.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lSjCxyPExD7l"
      },
      "source": [
        "### Lagrange dual of the standard form LP"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have seen above that the domain of the dual, namely $\\textbf{dom }g = \\{(\\lambda,\\nu):g(\\lambda,\\nu) > - \\infty\\}$, has often dimension smaller than $m+q$. It is often the case that we can describe $\\textbf{dom }g$ with a set of linear equality constraints. In this case it is often convenient to include the domain as an explicit constraint to the problem."
      ],
      "metadata": {
        "id": "xXZ2xk9M0sby"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XETjoYZXkQiR"
      },
      "source": [
        "We have already seen that the Lagrange dual of the standard form LP $(2)$ is given by $(3)$. So the Lagrange dual problem of the standard form LP is\n",
        "\n",
        "$$\n",
        "\\begin{array}{ll}\n",
        "\\max & g(\\lambda,\\nu) = \\left\\{\\begin{array}{ll}-b^\\top\\nu & A^\\top \\nu - \\lambda +c =0 \\\\ -\\infty & \\text{otherwise,}  \\end{array}\\right. \\\\\n",
        "s.t. & \\lambda \\succcurlyeq 0.  \n",
        "\\end{array}\\tag{6}\n",
        "$$\n",
        "\n",
        "We can form an equivalent problem by making the constraints explicit, namely\n",
        "\n",
        "$$\n",
        "\\begin{array}{ll}\n",
        "\\max & -b^\\top\\nu\\\\\n",
        "s.t. & A^\\top \\nu - \\lambda +c =0\\\\\n",
        " & \\lambda \\succcurlyeq 0  \n",
        "\\end{array}\\tag{7}\n",
        "$$\n",
        "\n",
        "or equivalently\n",
        "\n",
        "$$\n",
        "\\begin{array}{ll}\n",
        "\\max & -b^\\top\\nu\\\\\n",
        "s.t. & A^\\top \\nu  +c \\succcurlyeq 0, \n",
        "\\end{array}\\tag{8}\n",
        "$$\n",
        "\n",
        "which is an LP in inequality form (note the symmetry). For our definition of optimisation problem $(6)$, $(7)$ and $(8)$ are three equivalent, but different problems. Nonetheless, with abuse of terminology we will indicate either $(7)$ or $(8)$  as the Lagrange dual of $(2)$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eCGIttAskQk3"
      },
      "source": [
        "---\n",
        "\n",
        "**Exercise 6.4:** Find the Lagrange dual problem of the inequality form LP\n",
        "\n",
        "$$\n",
        "\\begin{array}{ll}\n",
        "\\min & c^\\top x \\\\\n",
        "s.t. & Ax \\preccurlyeq b.  \n",
        "\\end{array} \\tag{9}\n",
        "$$\n",
        "\n",
        "***EDIT THE FILE TO ADD YOUR PROOF HERE***\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kRTZM_Z7kQni"
      },
      "source": [
        "## 6.2.1 Weak and Strong duality"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BG8jCIfqkQqM"
      },
      "source": [
        "The optimal value of the Lagrange dual problem, which we indicate as $d^*$ is the best lower bound (**that can be found by Lagrange duality**). Thus, it is always true that \n",
        "\n",
        "$$\n",
        "d^* \\le p^*.\n",
        "$$\n",
        "\n",
        "This property is called **weak duality** and the difference $p^* - d^*$ is known as **optimal duality gap**. Note that weak duality holds even when the two optimal values are infinity. If the primal is unbounded below, then $p^*=-\\infty$ and we must have $d^*=-\\infty$ which implies that the dual is infeasible. If the dual is unbounded above, then $d^*=\\infty$ and we must have $p^*=\\infty$ which implies that the primal is infeasible. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q-HAXRoNox8q"
      },
      "source": [
        "Weak duality is very important because it easily provides us with certificates about how good an approximate solution is. In fact, since the dual problem is always convex, it can be easility solved even when the primal is not convex and cannot be solved. Then if we find a value of the primal using a heurisitic and that value is very close to $d^*$, then we have a certificate that our primal solution is actually a good one. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LQyO58yzoyAO"
      },
      "source": [
        "If the equality\n",
        "\n",
        "$$\n",
        "d^* = p^*\n",
        "$$\n",
        "\n",
        "holds, then we have that **strong duality** holds and, of course, the duality gap is zero. Strong duality does not hold in general, not even when the primal is convex. However, if the problem is convex there are results that establish additional conditions on the problem that, if satisfied, guarantee strong duality. These conditions are called **constraint qualifications**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Avv_6O9oyDO"
      },
      "source": [
        "One simple constraint qualification for convex problem is **Slater's condition**: *If there exists a feasible $x \\in \\textbf{int }\\mathcal{D}$ (interior of $\\mathcal{D}$), such that the inequality constraints hold strictly ($<$) (and the problem is convex), then strong duality holds*.\n",
        "\n",
        "Such a point, i.e. a feasible point $x \\in \\textbf{int }\\mathcal{D}$ that satifies the inequality constraints strictly, is called **strictly feasible**. \n",
        "\n",
        "Slater's condition can be refined. For instance the result hold if $x \\in \\textbf{relint } \\mathcal{D}$ (the interior relative to the affine hull) instead of $x \\in \\textbf{int }\\mathcal{D}$. Or, most importantly, the inequality does not have to hold strictly for linear inequality constraints. This means that for a convex problem with linear inequalities and $\\textbf{dom }f_0$ open, strong duality holds if the problem is feasible."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VmZWm45moyGD"
      },
      "source": [
        "Note that Slater's condition does not simply imply strong duality for convex problems. It also implies that the dual optimal is attained when $d^*>-\\infty$, i.e. there exists a dual feasible $(\\lambda^*, \\nu^*)$ such that $g(\\lambda^*, \\nu^*) = d^* = p^*$.\n",
        "\n",
        "Going back to the idea of duality as a certificate of how good our suboptimal solution is, we note that this certification can also be used as a stopping condition in iterative algorithms. For instance, we can compute iteratively suboptimal solutions for the primal and the dual and check how close the optimal values are, i.e. stop when $p^*-d^* < \\varepsilon$. If strong duality holds, then we make the tolerance $\\varepsilon$ as small as we want."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_rblTDezoyJu"
      },
      "source": [
        "### Examples"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L5JVKPmfoyMq"
      },
      "source": [
        "**LP:** For the inequality form LP $(9)$, strong duality holds if the primal is feasible. Of course the same holds for equality form LPs. Since we have seen that the dual of inequality form LPs are equality form LPs, and vice versa, then strong duality holds for LPs if the dual is feasible. We are left with the case in which both primal and dual are unfeasible, and in that case strong duality may fail. This can occur. \n",
        "\n",
        "**Entropy maximisation:** For problem $(4)$, strong duality holds if the primal is feasible."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GI-A30lioGdG"
      },
      "source": [
        "**Quadratic programme:** Consider the optimisation problem\n",
        "\n",
        "$$\n",
        "\\begin{array}{ll}\n",
        "\\min & x^\\top P x \\\\\n",
        "s.t. & Ax = b.  \n",
        "\\end{array}\n",
        "$$\n",
        "\n",
        "The dual function is $g(\\nu) = -\\frac{1}{4} \\nu^{\\top} A P^{-1} A^\\top \\nu - b^\\top \\lambda$ and the dual problem is\n",
        "\n",
        "$$\n",
        "\\begin{array}{ll}\n",
        "\\max & -\\frac{1}{4} \\nu^{\\top} A P^{-1} A^\\top \\nu - b^\\top \\nu.\n",
        "\\end{array}\n",
        "$$\n",
        "\n",
        "Using Slater's condition, strong duality holds if the primal is feasible, i.e. $p^*=d^*$ as long as $b\\in \\mathcal{R}(A)$, which means $p^* < \\infty$. In fact, for this problem one can show that duality always holds, even when  $p^*=\\infty$. This is the case when $b\\not \\in \\mathcal{R}(A)$, which implies that there exists a $z$ such that $A^\\top z = 0$, $b^\\top z \\not = 0$. It follows that the dual function is unbounded above along the line $\\{tz : t\\in\\mathbb{R} \\}$ and so $d^* = \\infty$ as well. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qBMDuiF3ab8Q"
      },
      "source": [
        "**Nonconvex quadratic problem with strong duality:** On some cases strong duality holds for nonconvex problems. This is the case, for instance, of \n",
        "\n",
        "$$\n",
        "\\begin{array}{ll}\n",
        "\\min & x^\\top A x + 2b^\\top x \\\\\n",
        "s.t. & x^\\top x \\le 1.  \n",
        "\\end{array}\n",
        "$$\n",
        "\n",
        "where $A\\in\\mathbb{S}^n$ but $A \\not \\succcurlyeq 0$, which makes it nonconvex. The dual problem is given by (prove it as an exercise)\n",
        "\n",
        "$$\n",
        "\\begin{array}{ll}\n",
        "\\max & -b^\\top (A+\\lambda I)^{\\dagger} b - \\lambda \\\\\n",
        "s.t. & A + \\lambda I \\succcurlyeq 0\\\\ \n",
        "& b \\in \\mathcal{R}(A+\\lambda I) \n",
        "\\end{array}\n",
        "$$\n",
        "\n",
        "It can be shown that the problem is equivalent to the SDP\n",
        "\n",
        "$$\n",
        "\\begin{array}{ll}\n",
        "\\max & -t - \\lambda \\\\\n",
        "s.t. & \\left[\\begin{array}{cc}A + \\lambda I & b\\\\ b^\\top & t \\end{array}\\right] \\succcurlyeq 0 \n",
        "\\end{array}\n",
        "$$\n",
        "\n",
        "Despite the primal is not convex, it is possible to show that strong duality holds. This is due to a more general, but difficult to prove, result that establishes that for any optimisation problem with quadratic objective and exactly one quadratic inequality constraint strong duality holds as long as Slater's condition holds. Thus, even though the primal is not convex, we can find the optimal value by solving the convex dual problem.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EUr2Mt2qoGf2"
      },
      "source": [
        "# 6.3 Geometric Interpretation"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import HTML\n",
        "HTML('<iframe width=\"850\" height=\"480\" src=\"https://www.youtube.com/embed/5NWVLIEaI0U\"></iframe>')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 505
        },
        "id": "w9COXeiGy0q7",
        "outputId": "672206cf-9da5-49d0-b4b4-152a59c55bd2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<iframe width=\"850\" height=\"480\" src=\"https://www.youtube.com/embed/5NWVLIEaI0U\"></iframe>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sKCFMXaGoGid"
      },
      "source": [
        "Duality can be interpreted and understood in several different ways: multicriterion interpretation, max-min characterisation, saddle-point interpretation, game interpretation and price/tax interpretation. The interested student can read more about this in Chapter $5.4$ of $[1]$, but that material is not part of this course.\n",
        "\n",
        "Instead, here we focus on the geometric interpretation. For simplicity, we consider a convex problem with objective function $f_0(x)$ and only one inequality constraint $f_1(x) \\le 0$ (but this can be extended to any number of inequality and equality constraints). Let $\\mathcal{G}=\\{(f_1(x),f_0(x)): x\\in\\mathcal{D}\\}$. The optimal value can be expressed in terms of $\\mathcal{G}$ as\n",
        "\n",
        "$$\n",
        "p^* = \\inf \\{t : (u,t) \\in \\mathcal{G},\\, u \\le 0 \\}.\n",
        "$$\n",
        "\n",
        "Then the dual function can be written as\n",
        "\n",
        "$$\n",
        "\\displaystyle g(\\lambda) = \\inf_{(u,t)\\in\\mathcal{G}} (t + \\lambda u).\n",
        "$$\n",
        "\n",
        "Hence, given $\\lambda$, we minimize $(\\lambda, 1)^\\top (u, t)$ over $\\mathcal{G}$. This yields a supporting\n",
        "hyperplane with slope $-\\lambda$. The intersection of this hyperplane with the $t$-axis gives $g(\\lambda)$. This is shown in the figure below.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div>\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1ZjaX9oRDQvm-Igfh2vC3LrI_r_GTisnd\" width=\"500\"/>\n",
        "</div>\n",
        "\n",
        "Figure 6.2. *Geometric interpretation of $g(\\lambda)$ as lower bound of $p^*$. Source: page $233$ of $[1]$.*"
      ],
      "metadata": {
        "id": "REk836kP0q1M"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PzEOONdBoGlO"
      },
      "source": [
        "Solving the dual problem is equivalent to finding the value of $\\lambda$ for which the duality gap is minimised. This is shown in the figure below."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div>\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1ohBvgvmMy2sqS04lzXSaJbE4eyR3l9VG\" width=\"500\"/>\n",
        "</div>\n",
        "\n",
        "Figure 6.3. *Supporting hyperplanes corresponding to three dual feasible values of $\\lambda$, including the optimum $\\lambda^*$. Strong duality does not hold; the optimal duality gap $p^* − d^*$ is positive. Source: page $233$ of $[1]$.*"
      ],
      "metadata": {
        "id": "cWqARt2F0zqA"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oAduWoDhoGqe"
      },
      "source": [
        "We can also explain strong duality using a geometric interpretation. Consider the set \n",
        "\n",
        "$$\n",
        "\\mathcal{A}=\\{(u,t) : f_1(x) \\le u,\\,f_0(x)\\le t \\text{ for some } x\\in\\mathcal{D}\\}.\n",
        "$$\n",
        "\n",
        "Since $\\mathcal{A}$ includes all the points in $\\mathcal{G}$ as well as the points which are worse, the set $\\mathcal{A}$ can be interpreted as a sort of epigraph form of $\\mathcal{G}$. Similarly to above, given $\\lambda$, we minimize $(\\lambda, 1)^\\top (u, t)$ over $\\mathcal{A}$. This yields a supporting hyperplane with slope $-\\lambda$. The intersection of this hyperplane with the $t$-axis gives $g(\\lambda)$. The figure below shows the set $\\mathcal{A}$."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div>\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=14qns6GEzP3U5NexiH6-1IIQNE67cwnfr\" width=\"500\"/>\n",
        "</div>\n",
        "\n",
        "Figure 6.4. *Source: page $234$ of $[1]$.*"
      ],
      "metadata": {
        "id": "xszQz9k9026r"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cXyojfrfoGtR"
      },
      "source": [
        "From the figure one can see that strong duality holds if and only if there exists a nonvertical supporting hyperplane to $\\mathcal{A}$ at its boundary point $(0, p^*)$. For convex problems $\\mathcal{A}$ is convex, hence it has a (possibly vertical) supporting hyperplace at $(0, p^*)$. Then Slater's condition simply requires the existence of a point $(\\tilde u, \\tilde t)\\in\\mathcal{A}$ such that $\\tilde u < 0$, i.e. a piece of $\\mathcal{A}$ is on the left of the $t$-axis.  If so, the supporting hyperplane at $(0, p^*)$ cannot be vertical (or it would cut the left part of $\\mathcal{A}$ off).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gIDJ6OA9oGwK"
      },
      "source": [
        "# 6.4 Optimality Conditions"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import HTML\n",
        "HTML('<iframe width=\"850\" height=\"480\" src=\"https://www.youtube.com/embed/5v198qjK-iI\"></iframe>')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 505
        },
        "id": "mib8D5q3y1qD",
        "outputId": "05e46ffc-7997-43f5-b781-e6a3b428a6eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<iframe width=\"850\" height=\"480\" src=\"https://www.youtube.com/embed/5v198qjK-iI\"></iframe>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "InGXubO-xD-E"
      },
      "source": [
        "Consider problem $(1)$. We do not assume that the problem is convex.\n",
        "\n",
        "Suppose that strong duality holds and the primal and dual optimal values $x^*$ and $(\\lambda^*,\\nu^*)$ are attained.  Then\n",
        "\n",
        "$$\n",
        "\\begin{array}{rl}\n",
        "f_0(x^*) &= g(\\lambda^*,\\nu^*) = \\inf_x \\left( f_0(x) + \\sum_{i=1}^m \\lambda_i^* f_i(x) +\\sum_{i=1}^p \\nu_i^* h_i(x) \\right)\n",
        "\\\\&\\le f_0(x^*) + \\sum_{i=1}^m \\lambda_i^* f_i(x^*) +\\sum_{i=1}^p \\nu_i^* h_i(x^*) \\le f_0(x^*),\n",
        "\\end{array}\n",
        "$$\n",
        "\n",
        "where the first equality is from strong duality, the second from the definition of dual function and the first inequality is from the infimum being smaller or equal than at any other point. The last inequality follows from the fact that since $x^*$ is optimal, then it is feasible and so the equality constraints sum to zero and the inequality constraints sum to a negative number. But since the first term of the chain is also the last term of the chain, it follows that all the inequality must hold with the equality sign. This has two important consequences: first, we conclude that $x^*$ minimizes $L(x, \\lambda^*, \\nu^*)$ over $x$. Second, it follows that\n",
        "\n",
        "$$\n",
        "\\sum_{i=1}^m \\lambda_i^* f_i(x^*) =0.\n",
        "$$\n",
        "\n",
        "Since each term of the sum is nonpositive, we conclude that \n",
        "\n",
        "$$\n",
        "\\lambda_i^* f_i(x^*) =0, \\qquad i=1,\\dots,m.\n",
        "$$\n",
        "\n",
        "This condition is called **complementary slackness** and implies that the $i$-th optimal Lagrange multiplier is zero unless\n",
        "the $i$-th constraint is active ($f_i(x^*)=0$) at the optimum.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-UIWcUMJzNH2"
      },
      "source": [
        "Assume now that all the functions are differentiable (without assuming convexity). Since $x^*$ minimises $L(x, \\lambda^*, \\nu^*)$ over $x$, it follows that its gradient must be zero at $x^*$. Thus we have the following necessary conditions for the optimal $x^*$ and $(\\lambda^*,\\nu^*)$ with zero duality gap:\n",
        "\n",
        "*   $f_i(x^*)\\le 0,\\qquad i=1,\\dots,m$\n",
        "*   $h_i(x^*)= 0,\\qquad i=1,\\dots,p$\n",
        "*   $\\lambda_i^* \\ge 0,\\qquad i=1,\\dots,m$\n",
        "*   $\\lambda_i^* f_i(x^*)=0,\\qquad i=1,\\dots,m$\n",
        "*   $\\nabla f_0(x^*) +  \\sum_{i=1}^m \\lambda_i^* \\nabla f_i(x^*) + \\sum_{i=1}^p  \\nu_i^* \\nabla h_i(x^*) =0$\n",
        "\n",
        "which are called **Karush-Kuhn-Tucker (KKT)** conditions.\n",
        "\n",
        "To summarize, for any optimisation problem with differentiable objective and constraint functions for which strong duality holds, any pair of primal and dual optimal points must satisfy the KKT conditions. "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**When the primal problem is convex, the KKT conditions are also sufficient for the points to be primal and dual optimal.**"
      ],
      "metadata": {
        "id": "6o15QmlbksXH"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8_G1zNJJzNLA"
      },
      "source": [
        "To see this, let $\\tilde x$, $\\tilde\\lambda$ and $\\tilde \\nu$ satisfy the KKT conditions. Note that the first two conditions state that $\\tilde x$ is primal feasible.\n",
        "Since $\\tilde \\lambda_i \\ge 0$, $L(x,\\tilde\\lambda,\\tilde \\nu)$ is convex in $x$; the last KKT condition states that $\\tilde x$ minimizes $L(x,\\tilde\\lambda,\\tilde \\nu)$\n",
        "over $x$. From this we conclude that\n",
        "\n",
        "$$\n",
        "g(\\tilde\\lambda,\\tilde \\nu) = L(\\tilde x,\\tilde\\lambda,\\tilde \\nu) \n",
        "= f_0(\\tilde x)\n",
        "$$\n",
        "\n",
        "because of complementary slackness and the fact that the equality constraints hold. This shows that $\\tilde x$, $\\tilde\\lambda$ and $\\tilde \\nu$ have zero duality gap and are primal and dual optimal. In summary, for any convex optimisation problem with differentiable objective and constraint functions, any points that satisfy the KKT conditions are primal and dual optimal, and have zero duality gap."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3kkP9sbDzNN7"
      },
      "source": [
        "The KKT conditions generalize the optimality condition $\\nabla f_0(x)=0$ for unconstrained problems and play an important role in optimisation. Many algorithms for convex optimisation are conceived as methods for solving the KKT conditions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-CdVLv57a2U8"
      },
      "source": [
        "## 6.4.1 Solving the Primal Problem via the Dual"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vhym-bZna2X8"
      },
      "source": [
        "We mentioned that if strong duality holds then $x^*$ minimises $L(x, \\lambda^*, \\nu^*)$ over $x$, where $(\\lambda^*, \\nu^*)$ is a dual optimal solution. This fact sometimes allows us to compute a primal optimal solution from a dual optimal solution. Suppose we have strong duality and an optimal $(\\lambda^*, \\nu^*)$ is known.\n",
        "Suppose that the minimizer of $L(x, \\lambda^*, \\nu^*)$, i.e. the solution of\n",
        "\n",
        "$$\n",
        "\\min_x \\,\\,\\, f_0(x) + \\sum_{i=1}^m \\lambda_i^* f_i(x) +\\sum_{i=1}^p \\nu_i^* h_i(x) \\tag{10}\n",
        "$$\n",
        "\n",
        "is unique (this occurs, for instance, when $L(x, \\lambda^*, \\nu^*)$ is strictly convex in $x$). Then if the solution of $(10)$ is primal feasible, it must be\n",
        "primal optimal; if it is not primal feasible, then no primal optimal point can exist, i.e., we can conclude that the primal optimum is not attained.\n",
        "\n",
        "This method is useful when solving the dual problem and then solving $\\nabla L(x, \\lambda^*, \\nu^*) = 0$ is easier than solving the primal problem (because the primal, for instance, is not convex)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hVLjtK3Ea2a8"
      },
      "source": [
        "---\n",
        "\n",
        "**Example 6.1:** (*Entropy maximisation*) Consider problem $(4)$ and its dual \n",
        "\n",
        "$$\n",
        "\\begin{array}{ll}\n",
        "\\max & -b^\\top \\lambda - \\nu - e^{-\\nu-1} \\sum_{i=1}^n e^{-a_i^\\top \\lambda} \\\\\n",
        "s.t. & \\lambda \\succcurlyeq 0 \n",
        "\\end{array} \n",
        "$$\n",
        "\n",
        "Suppose that Slater's condition holds and that we have solved the dual problem and found  $(\\lambda^*, \\nu^*)$. Then the Lagrangian at $(\\lambda^*, \\nu^*)$ is\n",
        "\n",
        "$$\n",
        "L(x, \\lambda^*, \\nu^*) = \\sum_{i=1}^n x_i \\log x_i +\\lambda^{*\\top}(Ax-b) + \\nu^*(\\textbf{1}^\\top x-1)\n",
        "$$\n",
        "\n",
        "which is strictly convex and bounded below. So we can compute the minimum by solving $\\nabla L(x, \\lambda^*, \\nu^*) = 0$, obtaining\n",
        "\n",
        "$$\n",
        "x_i^* = e^{-a_i^\\top \\lambda^* + \\nu^* + 1}, \\qquad i=1,\\dots, n\n",
        "$$\n",
        "\n",
        "If $x^*$ is primal feasible, it must be the optimal solution of the primal problem. If $x^*$ is not primal feasible, then we can conclude that the primal optimum is not attained.\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cBLMbcw2zNQt"
      },
      "source": [
        "# 6.5 Perturbation and Sensitivity Analysis"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import HTML\n",
        "HTML('<iframe width=\"850\" height=\"480\" src=\"https://www.youtube.com/embed/5l9aOdeL7Wc\"></iframe>')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 505
        },
        "id": "TmvSzpJVy2zW",
        "outputId": "bfb6db85-53b6-4639-ecbf-f015caf150b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<iframe width=\"850\" height=\"480\" src=\"https://www.youtube.com/embed/5l9aOdeL7Wc\"></iframe>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gNRQY9FwzNTX"
      },
      "source": [
        "Duality provides useful information about the sensitivity of the optimal value with respect to perturbations of certain constraints. This is very useful because in practical scenarios the values of the parameters in the problems are often approximations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I52lSCJezNWR"
      },
      "source": [
        "Consider the perturbed version of problem $(1)$, namely\n",
        "\n",
        "$$\n",
        "\\begin{array}{lll}\n",
        "\\min & f_0(x) &\\\\\n",
        "s.t. & f_i(x) \\le u_i, & i = 1,\\dots,m\\\\\n",
        "& h_i(x) = v_i,  & i = 1,\\dots,p \n",
        "\\end{array} \n",
        "$$\n",
        "\n",
        "This problem coincides with $(1)$ when $u=v=0$. We also give an intuitive explanation of the meaning of the perturbed inequality constraints. When $u_i$ is positive it means that we have relaxed the $i$-th inequality constraint; when $u_i$ is negative, it means that we have tightened the constraint. Let us define $p^*(u,v)$ as the optimal value of the perturbed problem. Clearly $p^*(0,0)=p^*$. If $p^*(u,v)=\\infty$, then the perturbed problem is unfeasible. Obviously, if the original problem is convex, so is the perturbed problem. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z633YV-wPqG3"
      },
      "source": [
        "Assume now that strong duality holds for the unperturbed problem and that the dual optimal is attained (for instance this is the case if the unperturbed problem is convex and Slater's condition holds). Let $(\\lambda^*,\\nu^*)$ be optimal for the dual of the unperturbed problem. Note that if $g(\\lambda,\\nu)$ is the dual function of the original problem, then $g(\\lambda,\\nu)-\\lambda u - \\nu v$ is the dual function of the perturbed problem. By weak duality of the perturbed problem\n",
        "\n",
        "$$\n",
        "p^*(u,v) \\ge g(\\lambda^*,\\nu^*) -\\lambda^* u - \\nu^* v\n",
        "$$\n",
        "\n",
        "But by strong duality of the original problem we have $g(\\lambda^*,\\nu^*)= p^*(0,0)$.\n",
        "\n",
        "Thus\n",
        "\n",
        "$$\n",
        "p^*(u,v) \\ge p^*(0,0)-\\lambda^{*\\top}u -\\nu^{*\\top}v. \\tag{11}\n",
        "$$\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i6mU3pjcPqKV"
      },
      "source": [
        "From this we have the following sentitivity conclusions:\n",
        "\n",
        "*   if $\\lambda_i^*$ is large: $p^*$ increases greatly if we thighten contraint $i$ ($u_i< 0$)\n",
        "*   if $\\lambda_i^*$ is small: $p^*$ does not decrease much if we loosen contraint $i$ ($u_i> 0$)\n",
        "*   if $\\nu_i^*$ is large and positive: $p^*$ increases greatly if we take $v_i< 0$\n",
        "*   if $\\nu_i^*$ is large and negative: $p^*$ increases greatly if we take $v_i> 0$\n",
        "*   if $\\nu_i^*$ is small and positive: $p^*$ does not decrease much if we take $v_i> 0$\n",
        "*   if $\\nu_i^*$ is small and negative: $p^*$ does not decrease much if we take $v_i< 0$\n",
        "\n",
        "\n",
        "The inequality $(11)$, and the conclusions listed above, give a lower bound on the perturbed optimal value, but no upper bound. For this reason the results are not symmetric with respect to loosening or tightening a constraint. For example, suppose that $\\lambda_i^*$ is large, and we loosen the $i$-th constraint a bit (i.e., take $u_i$ small and positive). In this case the inequality $(11)$ is not useful; it does not, for\n",
        "example, imply that the optimal value will decrease considerably.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ML46i0QCPqNs"
      },
      "source": [
        "Suppose now that $p^*(u,v)$ is differentiable at $u = 0$, $v = 0$. Then, provided strong\n",
        "duality holds, we have that\n",
        "\n",
        "$$\n",
        "\\lambda_i^* = -\\frac{\\partial p^*(0,0)}{\\partial u_i} \\qquad \\nu_i^* = -\\frac{\\partial p^*(0,0)}{\\partial v_i}.\n",
        "$$\n",
        "\n",
        "These expressions follow from $(11)$. In fact, let $u = t e_i$ with $e_i$ the $i$-th unit vector, and $v=0$, then if $t>0$\n",
        "\n",
        "$$\n",
        "\\frac{p^*(t e_i,0) - p^*(0,0)}{t} \\ge -\\lambda_i^{*}\n",
        "$$\n",
        "\n",
        "and taking the limit with $t\\to 0$ we have\n",
        "\n",
        "$$\n",
        "\\frac{\\partial p^*(0,0)}{\\partial u_i} \\ge -\\lambda_i^{*}.\n",
        "$$\n",
        "\n",
        "Doing the same for $t < 0$ we have the oppositve inequality. Doing the same for $v$ gives us the other derivative.\n",
        "\n",
        "So when the differentiability assumption holds, the optimal Lagrange multipliers are exactly the local sensitivities of the optimal value with respect to constraint perturbations. In contrast to the nondifferentiable case, this interpretation is symmetric: tightening the $i$-th inequality constraint by\n",
        "a small amount (i.e., taking $u_i$ small and negative) yields an increase in $p^*$ of\n",
        "approximately $−\\lambda^*_i u_i$; loosening the $i$-th constraint by a small amount (i.e., taking $u_i$\n",
        "small and positive) yields a decrease in $p^*$ of approximately $\\lambda^*_i u_i$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iL8aKPOePqQ5"
      },
      "source": [
        "The $i$-th optimal Lagrange multiplier tells us how active the constraint is: If $\\lambda_i^*$ is small, it means that the constraint can be loosened or tightened a bit without much effect on the optimal value; if $\\lambda_i^*$ is large, it means that if the constraint is loosened or tightened a bit, the effect on\n",
        "the optimal value will be great.\n",
        "\n",
        "For instance, consider the problem of designing a circuit in which the constraints are power and surface with $\\lambda_1^*=0.1$ associated to the power constraint and $\\lambda_2^*=100$ associated to the surface constraint. Then we see that if we get more or less power, we will not really improve or worsen the optimal design. However, if we are given less surface, we will definitely have a much worse design."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "giPVfSYiPqUO"
      },
      "source": [
        "# 6.6 Duality and Problem Reformulations"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import HTML\n",
        "HTML('<iframe width=\"850\" height=\"480\" src=\"https://www.youtube.com/embed/1MHTinR_aXg\"></iframe>')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 505
        },
        "id": "zpakDMNTy3yu",
        "outputId": "21596e63-eae9-408a-b95e-ea17064bc8c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<iframe width=\"850\" height=\"480\" src=\"https://www.youtube.com/embed/1MHTinR_aXg\"></iframe>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QikhNp-DPqXh"
      },
      "source": [
        "We already know that an optimisation problem can be formulated in several equivalent ways. From each way we can obtain a dual. It is important to stress that these duals are in general completely different. In other words, duals of the same primal written in different ways are not necessarily or obviously equivalent.\n",
        "\n",
        "This motivates us to explore primal problem reformulations for the sake of obtaining a better dual. One can consider the following types of reformulations:\n",
        "*    Introducing new variables and associated equality constraints.\n",
        "*    Replacing the objective function with an increasing function of the original objective.\n",
        "*    Making explicit constraints implicit, i.e., incorporating them into the domain.\n",
        "\n",
        "We now see a few examples."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jFimajwszNY9"
      },
      "source": [
        "## 6.6.1 Introducing New Variables"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YKPE0lTMgVrI"
      },
      "source": [
        "Consider the unconstrained problem\n",
        "\n",
        "$$\n",
        "\\begin{array}{ll}\n",
        "\\min & f_0(Ax+b).  \n",
        "\\end{array}\n",
        "$$\n",
        "\n",
        "Its Lagrangian dual function is $g = \\inf_x f_0(Ax+b) = p^*$, i.e. the dual is just a constant equal to the optimal value. This is not useful nor informative. Let us now consider the equivalent problem\n",
        "\n",
        "$$\n",
        "\\begin{array}{ll}\n",
        "\\min & f_0(y) \\\\\n",
        "s.t. & Ax +b = y.  \n",
        "\\end{array} \\tag{12}\n",
        "$$\n",
        "\n",
        "The dual problem is\n",
        "\n",
        "$$\n",
        "\\begin{array}{ll}\n",
        "\\max & b^\\top \\nu - f_0^*(\\nu) \\\\\n",
        "s.t. & A^\\top \\nu = 0.  \n",
        "\\end{array} \\tag{13}\n",
        "$$\n",
        "\n",
        "This dual is not anymore trivial and can be used for finding lower bounds or for sensitivity analysis.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oIB-mKR8ovW0"
      },
      "source": [
        "---\n",
        "\n",
        "**Exercise 6.5:** Show that $(13)$ is the dual of $(12)$.\n",
        "\n",
        "***EDIT THE FILE TO ADD YOUR PROOF HERE***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZYdBeGN2ovaf"
      },
      "source": [
        "**Exercise 6.6:** Consider the problem\n",
        "\n",
        "$$\n",
        "\\begin{array}{ll}\n",
        "\\min & \\displaystyle \\log \\left(\\sum_{i=1}^m e^{a_i^\\top x+b_i}\\right)  \n",
        "\\end{array}\n",
        "$$\n",
        "\n",
        "By introducing  the variable $y=Ax+b$ we obtain the equivalent problem\n",
        "\n",
        "$$\n",
        "\\begin{array}{ll}\n",
        "\\min & \\displaystyle \\log \\left(\\sum_{i=1}^m e^{y_i}\\right) \\\\\n",
        "s.t. & Ax +b = y.  \n",
        "\\end{array}\n",
        "$$\n",
        "\n",
        "Show that the dual is given by\n",
        "\n",
        "$$\n",
        "\\begin{array}{ll}\n",
        "\\max & \\displaystyle b^\\top \\nu - \\sum_{i=1}^m \\nu_i \\log \\nu_i \\\\\n",
        "s.t. & \\mathbf{1}^\\top \\nu = 1\\\\ \n",
        "& A^\\top \\nu =0 \\\\ \n",
        "& \\nu \\succcurlyeq 0.\n",
        "\\end{array}\n",
        "$$\n",
        "\n",
        "***EDIT THE FILE TO ADD YOUR PROOF HERE***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RXfaw3tHovd7"
      },
      "source": [
        "**Exercise 6.7:** Consider the problem\n",
        "\n",
        "$$\n",
        "\\begin{array}{ll}\n",
        "\\min & ||Ax -b||.\n",
        "\\end{array}\n",
        "$$\n",
        "\n",
        "The dual here is the trivial $p^*$. Determine an equivalent primal and a nontrivial dual.\n",
        "\n",
        "\n",
        "***EDIT THE FILE TO ADD YOUR PROOF HERE***\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yEB6eEFwovhj"
      },
      "source": [
        "## 6.6.2 Implicit Constraints"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hML3DHp_ovlF"
      },
      "source": [
        "The next simple reformulation that we study is the inclusion of some of the constraints in the objective function, by modifying the objective function to be infinite when the constraint is violated.\n",
        "\n",
        "Consider the LP with box contraints $l \\preccurlyeq x \\preccurlyeq u$\n",
        "\n",
        "$$\n",
        "\\begin{array}{ll}\n",
        "\\min & c^\\top x \\\\\n",
        "s.t. & Ax = b\\\\ \n",
        "& l \\preccurlyeq x \\preccurlyeq u.\n",
        "\\end{array}\n",
        "$$\n",
        "\n",
        "The dual of this problem is\n",
        "\n",
        "$$\n",
        "\\begin{array}{ll}\n",
        "\\max & -b^\\top \\nu - \\lambda_1^\\top u + \\lambda_2^\\top l \\\\\n",
        "s.t. & A^\\top \\nu +\\lambda_1 -\\lambda_2 +c = 0\\\\ \n",
        "& \\lambda_1 \\succcurlyeq 0, \\quad   \\lambda_2 \\succcurlyeq 0.\n",
        "\\end{array}\n",
        "$$\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KYZZjxRyovok"
      },
      "source": [
        "An alternative dual can be obtained as follows. We first define the equivalent primal as\n",
        "\n",
        "$$\n",
        "\\begin{array}{ll}\n",
        "\\min & f_0(x) \\\\\n",
        "s.t. & Ax = b\n",
        "\\end{array}\n",
        "$$\n",
        "\n",
        "where \n",
        "\n",
        "$$\n",
        "f_0(x) = \\left\\{\\begin{array}{ll} c^\\top x & l \\preccurlyeq x \\preccurlyeq u \\\\ \\infty & \\text{otherwise}\\end  {array}\\right.\n",
        "$$\n",
        "\n",
        "The dual function is given by\n",
        "\n",
        "$$\n",
        "g(\\nu) = \\inf_{l \\preccurlyeq x \\preccurlyeq u} (c^\\top x +\\nu^\\top (Ax-b)) = -b^\\top \\nu - u^\\top (A^\\top \\nu + c)^- +l^\\top (A^\\top \\nu + c)^+\n",
        "$$\n",
        "\n",
        "where $y_i^+ = \\max\\{y_i,0\\}$ and $y_i^- = \\max\\{-y_i,0\\}$. Thus the dual is\n",
        "\n",
        "$$\n",
        "\\max \\quad -b^\\top \\nu - u^\\top (A^\\top \\nu + c)^- +l^\\top (A^\\top \\nu + c)^+\n",
        "$$\n",
        "\n",
        "which is an unconstrained (piece-wise linear) problem."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fUeyNxe_ovsI"
      },
      "source": [
        "# 6.7 Generalised Inequalities"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import HTML\n",
        "HTML('<iframe width=\"850\" height=\"480\" src=\"https://www.youtube.com/embed/Mfq3fbqZ2vc\"></iframe>')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 505
        },
        "id": "25CJmq-Ny4oT",
        "outputId": "b964032f-b1fd-472e-9e82-8e161b60b549"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<iframe width=\"850\" height=\"480\" src=\"https://www.youtube.com/embed/Mfq3fbqZ2vc\"></iframe>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9T-uuZdXR1sJ"
      },
      "source": [
        "Duality extends straightforwardly to problems with generalised inequalities, i.e.\n",
        "\n",
        "$$\n",
        "\\begin{array}{lll}\n",
        "\\min & f_0(x) &\\\\\n",
        "s.t. & f_i(x) \\preccurlyeq_{K_i} 0, & i = 1,\\dots,m\\\\\n",
        "& h_i(x) = 0,  & i = 1,\\dots,p \n",
        "\\end{array} \n",
        "$$\n",
        "\n",
        "where $K_i \\subseteq \\mathbb{R}^{k^i}$ are proper cones.\n",
        "\n",
        "The Lagrangian and dual function are identical to the standard case, with the exception that the $\\lambda_i$'s are now vectors in $\\mathbb{R}^{k_i}$ instead of scalars. As in a problem with scalar inequalities, the dual function gives lower bounds on $p^*$. For a problem with scalar inequalities, we require $\\lambda_i \\ge 0$. Here the nonnegativity requirement on the dual\n",
        "variables is replaced by the condition\n",
        "\n",
        "$$\n",
        "\\lambda_i \\succcurlyeq_{K_i^*} 0, \\qquad i= 1,\\dots,m,\n",
        "$$\n",
        "\n",
        "where $K_i^*$ denotes the dual cone of $K_i$. In other words, the Lagrange multipliers associated with inequalities must be dual nonnegative. In particular note that $f_i(x) \\preccurlyeq_{K_i} 0$ and $\\lambda_i \\succcurlyeq_{K_i^*} 0$ imply $f_i(x) \\lambda_i \\le 0$. From this property all the rest of the duality theory follows: weak duality, strong duality with constraint qualification, complementary slackness, KKT conditions, perturbation and sensitivity analysis (with the only change that any inequality involving only $f_i(x)$ is $\\preccurlyeq_{K_i}$ instead of $\\le$ and any inequality involving only $\\lambda_i$ is $\\succcurlyeq_{K_i^*}$ instead of $\\ge$)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TMTwWuzGR1vD"
      },
      "source": [
        "---\n",
        "\n",
        "**Example 6.2:** (*Semidefinite programme in inequality form*) Consider the SDP\n",
        "\n",
        "$$\n",
        "\\begin{array}{ll}\n",
        "\\min & c^\\top x \\\\\n",
        "s.t. & x_1 F_1 + \\cdots + x_n F_n \\preccurlyeq G \n",
        "\\end{array} \n",
        "$$\n",
        "\n",
        "with $F_1$, ..., $F_n$, $G\\in\\mathbb{S}^k$. Define the Lagrange multiplier as the matrix $Z \\in\\mathbb{S}^k$. The Lagrangian is\n",
        "\n",
        "$$\n",
        "L(x,Z) = c^\\top x + \\textbf{tr}(Z(x_1 F_1 + \\cdots + x_n F_n - G))\n",
        "$$\n",
        "\n",
        "and the dual function is given by\n",
        "\n",
        "$$\n",
        "g(Z) = \\inf_x L(x,Z)  = \\left\\{\\begin{array}{lll}-\\textbf{tr}(GZ) & \\textbf{tr}(F_iZ)+c_i = 0& i=1,\\dots,n\\\\-\\infty & \\text{otherwise.}&\\end{array}\\right.\n",
        "$$\n",
        "\n",
        "So the dual problem is \n",
        "\n",
        "$$\n",
        "\\begin{array}{llll}\n",
        "\\max & -\\textbf{tr}(GZ) &&\\\\\n",
        "s.t. & Z \\succcurlyeq 0, & \\textbf{tr}(F_iZ)+c_i = 0& i=1,\\dots,n.\n",
        "\\end{array} \n",
        "$$\n",
        "\n",
        "If the primal SDP problem is strictly feasible (i.e. $x_1 F_1 + \\cdots + x_n F_n - G \\prec 0 $), then $p^* = d^*$.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BD1Gn4jWR17L"
      },
      "source": [
        "# End of CHAPTER 6"
      ]
    }
  ]
}